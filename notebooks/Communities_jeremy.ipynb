{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89df1666",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T20:55:00.737020Z",
     "start_time": "2023-12-30T20:55:00.695877Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "import community\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebc8b9d04dc43c",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "### Loading of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc598690abe1b7e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T20:55:02.070064Z",
     "start_time": "2023-12-30T20:55:02.030200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download names to make it more friendly\n",
    "json_file_path_1 = '../data/all_coins_by_mc_1.json'\n",
    "json_file_path_2 = '../data/all_coins_by_mc_2.json'\n",
    "\n",
    "# Open the JSON file and load its contents\n",
    "with open(json_file_path_1, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "with open(json_file_path_2, 'r') as file:\n",
    "    data = data + json.load(file)\n",
    "# Now, 'data' contains the contents of the JSON file as a Python object (dictionary, list, etc.)\n",
    "names_list = [stock[\"name\"] for stock in data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7b3ac4",
   "metadata": {},
   "source": [
    "### Load price data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab398b5bbcc7498f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T20:55:09.738571Z",
     "start_time": "2023-12-30T20:55:06.968350Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/processed/prices.csv\", parse_dates=['date'])\n",
    "data.rename(columns={data.columns[0]: 'date'}, inplace=True)\n",
    "data.set_index(\"date\", inplace=True)\n",
    "data.columns = names_list\n",
    "data = data.loc['2021-01-01':'2022-12-31'] # Filter\n",
    "data = data.dropna(axis=1, how='all') # Drop all columns that contains only null value\n",
    "data = data.dropna(axis=1, thresh=0.95*len(data.index)) # Drop columns that contains more than 25% of null value\n",
    "data = data.ffill() # ffill null values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f712dd",
   "metadata": {},
   "source": [
    "### Compute and normalize Hourly log-return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a6d984",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_returns_pct = data.pct_change()\n",
    "hourly_returns_pct = hourly_returns_pct.iloc[1:]\n",
    "hourly_returns_pct.to_csv(\"../data/processed/hourly_return.csv\")\n",
    "hourly_log_returns = np.log(hourly_returns_pct + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "hourly_log_returns = pd.DataFrame(scaler.fit_transform(hourly_log_returns), columns=hourly_log_returns.columns, index=hourly_log_returns.index)\n",
    "hourly_log_returns.to_csv(\"../data/processed/normalized_log_ret.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a2aff7",
   "metadata": {},
   "source": [
    "## Eigenvalues of correlation matrix analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa9c42",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (12,6))\n",
    "sns.histplot(hourly_log_returns.values.flatten(), stat='density', bins=2000, alpha=0.5, color=\"blue\", edgecolor='None')\n",
    "sns.histplot(np.random.normal(0,1,1000000), stat='density', alpha=0.5, bins = 30, color=\"orange\", edgecolor='None')\n",
    "#sns.kdeplot(np.random.normal(0,1,1000000), fill=True, color=\"orange\", alpha=0.5, label=\"Column2\")\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlim(-25, 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P0(lambdas,q):\n",
    "    lambda_plus = (1+np.sqrt(q))**2\n",
    "    lambda_minus = (1-np.sqrt(q))**2\n",
    "    vals = 1/(q*2*np.pi*lambdas)*np.sqrt((lambda_plus-lambdas)*(lambdas-lambda_minus))\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d866c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=266\n",
    "T=12000\n",
    "R=np.random.normal(0,1,N*T).reshape((N,T))\n",
    "R\n",
    "R.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5971f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "C=np.corrcoef(R)\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4588b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_e, V_e = LA.eig(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d3c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=N/T\n",
    "lambdas=np.linspace((1.-np.sqrt(q))**2,(1.+np.sqrt(q))**2,200)\n",
    "P0s=P0(lambdas,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c16dd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(lambdas_e[1:],kde=False, norm_hist=True,bins=50)  # no Kernel Density Estimation\n",
    "plt.plot(lambdas,P0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1078bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = hourly_log_returns[1000:1720]\n",
    "(T,N) = df_plot.shape\n",
    "(T,N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b18d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "T/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d612451",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = df_plot.corr().to_numpy()\n",
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac98c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_e, V_e = LA.eig(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe82326",
   "metadata": {},
   "outputs": [],
   "source": [
    "q=N/T\n",
    "lambdas=np.linspace((1.-np.sqrt(q))**2,(1.+np.sqrt(q))**2,200)\n",
    "P0s=P0(lambdas,q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32818279",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(lambdas_e[1:],kde=False, norm_hist=True,bins=200)  # no Kernel Density Estimation\n",
    "plt.plot(lambdas,P0s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8af05",
   "metadata": {},
   "source": [
    "## Network creation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c4a4b",
   "metadata": {},
   "source": [
    "### Filtering based on RMT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ebd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_C_minus_C0(lambdas, v, lambda_plus, removeMarketMode=False):\n",
    "    N=len(lambdas)\n",
    "    C_clean=np.zeros((N, N))\n",
    "\n",
    "    order = np.argsort(lambdas)\n",
    "    lambdas,v = lambdas[order],v[:,order]\n",
    "\n",
    "    v_m=np.matrix(v)\n",
    "\n",
    "    # note that the eivenvalues are sorted\n",
    "    for i in range(1*removeMarketMode,N):\n",
    "        if lambdas[i]>lambda_plus:\n",
    "            C_clean=C_clean+lambdas[i] * np.dot(v_m[:,i],v_m[:,i].T)\n",
    "    return C_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3354d5a",
   "metadata": {},
   "source": [
    "### From C (as in class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26402954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_network(R, mst=False):   # R is a matrix of return\n",
    "    \n",
    "    N=R.shape[1]\n",
    "    T=R.shape[0]\n",
    "\n",
    "    q=N*1./T\n",
    "    lambda_plus=(1.+np.sqrt(q))**2\n",
    "    C=R.corr()\n",
    "    lambdas, v = LA.eigh(C)\n",
    "    C_s=compute_C_minus_C0(lambdas,v,lambda_plus)\n",
    "    C_s = np.abs(C_s)\n",
    "\n",
    "    return nx.from_numpy_array(C_s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ad14e",
   "metadata": {},
   "source": [
    "### Minimum Spanning Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mst_network(R):\n",
    "    N=R.shape[1]\n",
    "    T=R.shape[0]\n",
    "\n",
    "    q=N*1./T\n",
    "    lambda_plus=(1.+np.sqrt(q))**2\n",
    "    C=R.corr()\n",
    "    lambdas, v = LA.eigh(C)\n",
    "    C_s=compute_C_minus_C0(lambdas,v,lambda_plus)\n",
    "    \n",
    "    # Compute distance matrix\n",
    "    D = np.sqrt(2*(1-C_s))\n",
    "    # Compute MST\n",
    "    G = nx.from_numpy_array(D)\n",
    "    return nx.minimum_spanning_tree(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4b5f2b",
   "metadata": {},
   "source": [
    "### Threshold network\n",
    "- Compute C and D\n",
    "- Create G from D\n",
    "- Remove all edge with weight smaller than *threshold*\n",
    "- Louvain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_network(R, threshold=0.95):\n",
    "    N = R.shape[1]\n",
    "    T = R.shape[0]\n",
    "    \n",
    "    # Compute lambda_plus and correlation matrix\n",
    "    q = N * 1. / T\n",
    "    lambda_plus = (1. + np.sqrt(q)) ** 2\n",
    "    C = R.corr()\n",
    "\n",
    "    # Compute distance matrix\n",
    "    D = np.sqrt(2*(1-C.to_numpy()))    \n",
    "    \n",
    "    # Compute graph\n",
    "    G = nx.from_numpy_array(D)\n",
    "    \n",
    "    G_copy = G.copy()\n",
    "\n",
    "    # Get the edge weights as a dictionary\n",
    "    edge_weights = nx.get_edge_attributes(G_copy, 'weight')\n",
    "\n",
    "    # Calculate the threshold for the top 5% of edges\n",
    "    threshold = sorted(edge_weights.values())[int(threshold * len(edge_weights))]\n",
    "    # Identify edges to remove based on the threshold\n",
    "    edges_to_remove = [(u, v) for (u, v, w) in G_copy.edges(data='weight') if w < threshold]\n",
    "    \n",
    "    G.remove_edges_from(edges_to_remove)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582a4a8",
   "metadata": {},
   "source": [
    "### Planar Maximally Filtered Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_graph_edges(G):\n",
    "    sorted_edges = []\n",
    "    for source, dest, data in sorted(G.edges(data=True), key=lambda x: x[2]['weight']):\n",
    "        sorted_edges.append({'source': source,\n",
    "                             'dest': dest,\n",
    "                             'weight': data['weight']})\n",
    "        \n",
    "    return sorted_edges\n",
    "\n",
    "def compute_PMFG(sorted_edges, nb_nodes):\n",
    "    PMFG = nx.Graph()\n",
    "    for edge in sorted_edges:\n",
    "        PMFG.add_edge(edge['source'], edge['dest'])\n",
    "        if not nx.is_planar(PMFG):\n",
    "            PMFG.remove_edge(edge['source'], edge['dest'])\n",
    "            \n",
    "        if len(PMFG.edges()) == 3*(nb_nodes-2):\n",
    "            print('test')\n",
    "            break\n",
    "    \n",
    "    return PMFG\n",
    "\n",
    "\n",
    "def pmfg_network(R):\n",
    "    N = R.shape[1]\n",
    "    T = R.shape[0]\n",
    "    \n",
    "    # Compute lambda_plus and correlation matrix\n",
    "    q = N * 1. / T\n",
    "    lambda_plus = (1. + np.sqrt(q)) ** 2\n",
    "    C = R.corr()\n",
    "    lambdas, v = LA.eigh(C)\n",
    "    C = compute_C_minus_C0(lambdas, v, lambda_plus)\n",
    "\n",
    "    # Compute distance matrix\n",
    "    D = np.sqrt(2*(1-C))    \n",
    "    \n",
    "    # Compute graph\n",
    "    G = nx.from_numpy_array(D)\n",
    "    \n",
    "    sorted_edges = sort_graph_edges(G)\n",
    "    \n",
    "    return compute_PMFG(sorted_edges, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b874c83",
   "metadata": {},
   "source": [
    "## Louvain clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74a7ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def louvain_clustering(G, resolution):\n",
    "    return  community.community_louvain.best_partition(G, resolution=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b981a53",
   "metadata": {},
   "source": [
    "## Rolling window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9423506849cb26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-30T20:55:17.628520Z",
     "start_time": "2023-12-30T20:55:17.602114Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/processed/normalized_log_ret.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354b83d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_asset(R, partition):\n",
    "    dict_cluster = {}\n",
    "    all_names = list(R.columns)\n",
    "    for i, name in enumerate(all_names):\n",
    "        dict_cluster[name]=partition[i]\n",
    "        \n",
    "    return dict_cluster\n",
    "\n",
    "def clustering(R: pd.DataFrame, period: int=720, interval: int = 24, method:str='baseline') -> dict:\n",
    "    \"\"\"Compute the clusters in a rolling window manner for the return matrix R.\n",
    "\n",
    "    Args:\n",
    "        R (pd.Dataframe): Normalized log-return matrix\n",
    "        period (int, optional): time period_. Defaults to 30 days (720 hours).\n",
    "        interval (int, optional): time interval. Defaults to 1 day (24 hours).\n",
    "        method (str, optional): Clustering method. Defaults to 'baseline'. Can be 'baseline', 'mst' 'threshold', 'pmf'.\n",
    "    \"\"\"\n",
    "    cluster_dict = {}\n",
    "    resolution = 1\n",
    "    for t0 in tqdm(range(0, len(R.index)-period, interval)):\n",
    "        R_tmp = R.iloc[t0:t0+period]\n",
    "        if method == 'baseline':\n",
    "            G = baseline_network(R_tmp)\n",
    "            resolution = 1.02\n",
    "        elif method == 'mst':\n",
    "            G = mst_network(R_tmp)\n",
    "            resolution = TODO\n",
    "        elif method == 'threshold':\n",
    "            G = threshold_network(R_tmp, threshold=0.95)\n",
    "            resolution = 1.2\n",
    "        elif method == 'pmf':\n",
    "            G = pmfg_network(R_tmp)\n",
    "            resolution = TODO\n",
    "        else:\n",
    "            raise ValueError(\"Method not recognized\")\n",
    "            \n",
    "        \n",
    "        cluster_dict[(t0, t0+period)] = rename_asset(R_tmp, louvain_clustering(G, resolution))\n",
    "            \n",
    "    return cluster_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5937b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███████████████▍                         | 189/501 [02:40<03:41,  1.41it/s]"
     ]
    }
   ],
   "source": [
    "clusters = clustering(df.set_index('date'), period=720, interval=24, method='baseline')\n",
    "clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3182a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the clusters\n",
    "name = \"baseline_720_24_1dot02.csv\"\n",
    "hourly_log_returns.to_csv(\"../data/processed/\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect first n cluster\n",
    "\n",
    "n = 10\n",
    "for i in range(n):\n",
    "    dict_clusters = clusters[list(clusters.keys())[i]]\n",
    "    grouped_cryptos = {}\n",
    "\n",
    "    for crypto, number in dict_clusters.items():\n",
    "        if number not in grouped_cryptos:\n",
    "            grouped_cryptos[number] = []\n",
    "        grouped_cryptos[number].append(crypto)\n",
    "\n",
    "    # Convert the dictionary values to a list for easier usage\n",
    "    result_lists = list(grouped_cryptos.values())\n",
    "\n",
    "    # Print or use the result_lists as needed\n",
    "    print(\"Clustering {} \\n\".format(i + 1))\n",
    "    for c in range(len(result_lists)):\n",
    "        print(result_lists[c])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38dded3",
   "metadata": {},
   "source": [
    "## Stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rand_score(clusters):\n",
    "    nb_clusters = len(clusters.keys())\n",
    "    rand_score = []\n",
    "    for i in range(nb_clusters - 1):\n",
    "        score = adjusted_rand_score(list(clusters[list(clusters.keys())[i]].values()), list(clusters[list(clusters.keys())[i+1]].values()))\n",
    "        rand_score.append(score)\n",
    "    return rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_clusters(clusters):\n",
    "    nb_clusters = len(clusters.keys())\n",
    "    list_nb_clusters = []\n",
    "    for i in range(nb_clusters - 1):\n",
    "        nb = max(clusters[list(clusters.keys())[i]].values())\n",
    "        list_nb_clusters.append(nb)\n",
    "    return list_nb_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dbb1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_nb_clusters(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def81989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nb of clusters\n",
    "plt.boxplot(compute_nb_clusters(clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0aae09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stability of the clusters\n",
    "plt.boxplot(compute_rand_score(clusters))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c75f6e7",
   "metadata": {},
   "source": [
    "## Markowitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d58b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representatives(clusters):\n",
    "    nb_clustering = len(clusters.keys())\n",
    "    representatives = []\n",
    "    for i in range(nb_clustering):\n",
    "        \n",
    "        current_representatives = []\n",
    "        current_cluster = clusters[list(clusters.keys())[i]]\n",
    "\n",
    "        # Loop through each community\n",
    "        for group in range(max(current_cluster.values())):\n",
    "            subset = [cluster[0] for cluster in current_cluster.items() if cluster[1] == group]\n",
    "            # Subset the data for the current group\n",
    "            subset_data = df[subset]\n",
    "\n",
    "            imputer = SimpleImputer(strategy='mean')\n",
    "            subset_data_imputed = pd.DataFrame(imputer.fit_transform(subset_data), columns=subset_data.columns, index=subset_data.index)\n",
    "\n",
    "            # Standardize/Normalize the data\n",
    "            scaler = StandardScaler()\n",
    "            subset_data_standardized = scaler.fit_transform(subset_data_imputed)\n",
    "\n",
    "            # Apply PCA\n",
    "            pca = PCA()\n",
    "            principal_components = pca.fit_transform(subset_data_standardized)\n",
    "\n",
    "            # Identify the leading coin (biggest contributor in the first principal component)\n",
    "            leading_coin_index = np.argmax(np.abs(pca.components_[0]))\n",
    "            leading_coin = subset_data.columns[leading_coin_index]\n",
    "            current_representatives.append(leading_coin)\n",
    "            # Print\n",
    "        representatives.append(current_representatives)\n",
    "    return representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8b744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "representatives = find_representatives(clusters)\n",
    "representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff5610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter([a for r in representatives for a in r]).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6edab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(representatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e51e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_sample_risks(clusters):\n",
    "\n",
    "    nb_clustering = len(clusters.keys())\n",
    "    vol = []\n",
    "    vol_abs = []\n",
    "    for i in range(nb_clustering):\n",
    "        length_window = list(clusters.keys())[i][1] - list(clusters.keys())[i][0]\n",
    "\n",
    "        corr_mat_insample = df.loc[list(clusters.keys())[i][0]:list(clusters.keys())[i][1],representatives[i]].corr()\n",
    "        corr_mat_outsample = df.loc[list(clusters.keys())[i][1]:list(clusters.keys())[i][1] + length_window, representatives[i]].corr()\n",
    "\n",
    "        inv_corr_insample = LA.inv(corr_mat_insample.to_numpy())\n",
    "        w_opt = inv_corr_insample @ np.ones(len(inv_corr_insample)) / (np.ones(len(inv_corr_insample)) @ inv_corr_insample @ np.ones(len(inv_corr_insample)))\n",
    "\n",
    "        realized_vol = w_opt @ corr_mat_insample @ w_opt.T\n",
    "        out_sample_vol = w_opt @ corr_mat_outsample @ w_opt.T\n",
    "        vol.append((realized_vol, out_sample_vol))\n",
    "        vol_abs.append(abs(realized_vol-out_sample_vol))\n",
    "    return vol, vol_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100a0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "vol, vol_abs = out_sample_risks(clusters)\n",
    "vol_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc052048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMT out_sample risks\n",
    "\n",
    "nb_clustering = len(clusters.keys())\n",
    "vol_rmt = []\n",
    "vol_abs_rmt = []\n",
    "for i in range(nb_clustering):\n",
    "    length_window = list(clusters.keys())[i][1] - list(clusters.keys())[i][0]\n",
    "\n",
    "    N=df.shape[1]\n",
    "    T=length_window\n",
    "\n",
    "    q=N*1./T\n",
    "    lambda_plus=(1.+np.sqrt(q))**2\n",
    "    \n",
    "    corr_mat_insample = df.loc[list(clusters.keys())[i][0]:list(clusters.keys())[i][1],:].drop('date', axis=1).corr()\n",
    "    lambdas, v = LA.eigh(corr_mat_insample)\n",
    "    corr_mat_insample=np.array(compute_C_minus_C0(lambdas,v,lambda_plus))\n",
    "    \n",
    "    corr_mat_outsample = df.loc[list(clusters.keys())[i][1]:list(clusters.keys())[i][1] + length_window, :].drop('date', axis=1).corr()\n",
    "    lambdas, v = LA.eigh(corr_mat_outsample)\n",
    "    corr_mat_outsample=np.array(compute_C_minus_C0(lambdas,v,lambda_plus))\n",
    "        \n",
    "    inv_corr_insample = LA.inv(corr_mat_insample)\n",
    "    w_opt = inv_corr_insample @ np.ones(len(inv_corr_insample)) / (np.ones(len(inv_corr_insample)) @ inv_corr_insample @ np.ones(len(inv_corr_insample)))\n",
    "\n",
    "    realized_vol = w_opt @ corr_mat_insample @ w_opt.T\n",
    "    out_sample_vol = w_opt @ corr_mat_outsample @ w_opt.T\n",
    "    vol_rmt.append((realized_vol, out_sample_vol))\n",
    "    vol_abs_rmt.append(abs(realized_vol-out_sample_vol))\n",
    "vol_abs_rmt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
